import 'dart:io';
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:tflite_flutter/tflite_flutter.dart';
import '../../utils/logger.dart';

/// TensorFlow Lite model manager with hardware acceleration
class TFLiteManager {
  static final _logger = AppLogger('TFLiteManager');
  static final _instance = TFLiteManager._internal();

  factory TFLiteManager() => _instance;
  TFLiteManager._internal();

  // Model interpreters cache
  final Map<String, Interpreter> _interpreters = {};
  final Map<String, IsolateInterpreter> _isolateInterpreters = {};

  // Hardware acceleration options
  InterpreterOptions? _defaultOptions;
  bool _useGpuDelegate = false;
  bool _useNnApiDelegate = false;
  bool _useCoreMLDelegate = false;
  bool _useXNNPackDelegate = false;

  /// Initialize TFLite manager with hardware acceleration
  Future<void> initialize() async {
    try {
      _logger.info('Initializing TFLite Manager');

      // Configure hardware acceleration based on platform
      await _configureHardwareAcceleration();

      _logger.info('TFLite Manager initialized with acceleration: '
          'GPU=$_useGpuDelegate, NNAPI=$_useNnApiDelegate, '
          'CoreML=$_useCoreMLDelegate, XNNPack=$_useXNNPackDelegate');
    } catch (e, stack) {
      _logger.error('Failed to initialize TFLite Manager',
          error: e, stackTrace: stack);
      // Don't throw - allow fallback to CPU
    }
  }

  /// Configure hardware acceleration based on platform capabilities
  Future<void> _configureHardwareAcceleration() async {
    _defaultOptions = InterpreterOptions();

    if (Platform.isAndroid) {
      await _configureAndroidAcceleration();
    } else if (Platform.isIOS) {
      await _configureIOSAcceleration();
    }

    // XNNPack delegate works on all platforms for CPU optimization
    await _configureXNNPackDelegate();
  }

  /// Configure Android-specific acceleration
  Future<void> _configureAndroidAcceleration() async {
    // Try GPU delegate first (preferred for image/vision models)
    try {
      final gpuDelegate = GpuDelegateV2(
        options: GpuDelegateOptionsV2(
          isPrecisionLossAllowed: true,
          inferencePreference: TfLiteGpuInferenceUsage.fastSingleAnswer,
          inferencePriority1: TfLiteGpuInferencePriority.minLatency,
          inferencePriority2: TfLiteGpuInferencePriority.auto,
          inferencePriority3: TfLiteGpuInferencePriority.auto,
        ),
      );

      _defaultOptions!.addDelegate(gpuDelegate);
      _useGpuDelegate = true;
      _logger.info('Android GPU delegate enabled');
    } catch (e) {
      _logger.warning('GPU delegate not available: $e');

      // Fallback to NNAPI (Neural Networks API)
      try {
        // NNAPI delegate is built-in on Android 8.1+ (API 27+)
        // Note: In newer versions of tflite_flutter, this might need adjustment
        _defaultOptions!.threads = 4; // Use multiple threads
        _useNnApiDelegate = true;
        _logger.info('Android NNAPI fallback enabled with 4 threads');
      } catch (e) {
        _logger.warning('NNAPI not available: $e');
      }
    }
  }

  /// Configure iOS-specific acceleration
  Future<void> _configureIOSAcceleration() async {
    try {
      // Core ML delegate for iOS
      // Note: Core ML delegate requires specific setup in tflite_flutter
      // For now, we'll use Metal delegate which is more widely supported
      final metalDelegate = GpuDelegate(
        options: GpuDelegateOptions(
          allowPrecisionLoss: true,
          waitType: TFLGpuDelegateWaitType.passive,
        ),
      );

      _defaultOptions!.addDelegate(metalDelegate);
      _useCoreMLDelegate = true;
      _logger.info('iOS Metal delegate enabled');
    } catch (e) {
      _logger.warning('Metal delegate not available: $e');
      // Fallback to multi-threaded CPU
      _defaultOptions!.threads = 4;
    }
  }

  /// Configure XNNPack delegate for optimized CPU operations
  Future<void> _configureXNNPackDelegate() async {
    try {
      // XNNPack is a highly optimized library for floating-point operations
      _defaultOptions!.threads = Platform.numberOfProcessors.clamp(2, 4);
      _useXNNPackDelegate = true;
      _logger.info('XNNPack delegate enabled with ${_defaultOptions!.threads} threads');
    } catch (e) {
      _logger.warning('XNNPack configuration failed: $e');
    }
  }

  /// Load a TFLite model from file path
  Future<Interpreter> loadModel(String modelPath, {String? modelId}) async {
    final id = modelId ?? modelPath;

    // Check cache first
    if (_interpreters.containsKey(id)) {
      _logger.info('Model $id already loaded from cache');
      return _interpreters[id]!;
    }

    try {
      _logger.info('Loading model from: $modelPath');

      // Create interpreter with hardware acceleration options
      final interpreter = await Interpreter.fromFile(
        File(modelPath),
        options: _defaultOptions,
      );

      // Cache the interpreter
      _interpreters[id] = interpreter;

      // Log model info
      _logModelInfo(interpreter, id);

      return interpreter;
    } catch (e, stack) {
      _logger.error('Failed to load model: $modelPath',
          error: e, stackTrace: stack);

      // Try loading without hardware acceleration as fallback
      try {
        _logger.info('Retrying without hardware acceleration');
        final interpreter = await Interpreter.fromFile(
          File(modelPath),
          options: InterpreterOptions()..threads = 2,
        );
        _interpreters[id] = interpreter;
        return interpreter;
      } catch (e2) {
        throw Exception('Failed to load model even without acceleration: $e2');
      }
    }
  }

  /// Load a TFLite model from assets
  Future<Interpreter> loadModelFromAsset(String assetPath, {String? modelId}) async {
    final id = modelId ?? assetPath;

    // Check cache first
    if (_interpreters.containsKey(id)) {
      _logger.info('Model $id already loaded from cache');
      return _interpreters[id]!;
    }

    try {
      _logger.info('Loading model from asset: $assetPath');

      // Load model bytes from assets
      final modelBytes = await rootBundle.load(assetPath);
      final modelBuffer = modelBytes.buffer.asUint8List();

      // Create interpreter
      final interpreter = await Interpreter.fromBuffer(
        modelBuffer,
        options: _defaultOptions,
      );

      // Cache the interpreter
      _interpreters[id] = interpreter;

      // Log model info
      _logModelInfo(interpreter, id);

      return interpreter;
    } catch (e, stack) {
      _logger.error('Failed to load model from asset: $assetPath',
          error: e, stackTrace: stack);
      rethrow;
    }
  }

  /// Create an isolate interpreter for background processing
  Future<IsolateInterpreter> createIsolateInterpreter(
    String modelPath, {
    String? modelId,
  }) async {
    final id = modelId ?? modelPath;

    // Check cache first
    if (_isolateInterpreters.containsKey(id)) {
      _logger.info('Isolate interpreter $id already exists');
      return _isolateInterpreters[id]!;
    }

    try {
      _logger.info('Creating isolate interpreter for: $modelPath');

      // Create isolate interpreter
      final interpreter = await IsolateInterpreter.create(
        address: InterpreterAddress.fromFile(File(modelPath)),
        options: _defaultOptions,
      );

      // Cache the interpreter
      _isolateInterpreters[id] = interpreter;

      return interpreter;
    } catch (e, stack) {
      _logger.error('Failed to create isolate interpreter',
          error: e, stackTrace: stack);
      rethrow;
    }
  }

  /// Run inference on a model
  Future<List<dynamic>> runInference(
    String modelId,
    List<dynamic> inputs,
  ) async {
    final interpreter = _interpreters[modelId];
    if (interpreter == null) {
      throw Exception('Model $modelId not loaded');
    }

    try {
      // Allocate tensors if needed
      interpreter.allocateTensors();

      // Set inputs
      for (int i = 0; i < inputs.length; i++) {
        interpreter.setTensor(i, inputs[i]);
      }

      // Run inference
      interpreter.invoke();

      // Get outputs
      final outputs = <dynamic>[];
      for (int i = 0; i < interpreter.getOutputTensors().length; i++) {
        outputs.add(interpreter.getOutputTensor(i));
      }

      return outputs;
    } catch (e, stack) {
      _logger.error('Inference failed for model $modelId',
          error: e, stackTrace: stack);
      rethrow;
    }
  }

  /// Run inference in isolate (non-blocking)
  Future<List<dynamic>> runInferenceInIsolate(
    String modelId,
    List<dynamic> inputs,
  ) async {
    final interpreter = _isolateInterpreters[modelId];
    if (interpreter == null) {
      throw Exception('Isolate interpreter $modelId not loaded');
    }

    try {
      // Prepare output buffers based on output shape
      final outputShapes = interpreter.getOutputTensors()
          .map((t) => t.shape)
          .toList();

      final outputs = outputShapes.map((shape) {
        final size = shape.reduce((a, b) => a * b);
        return List.filled(size, 0.0);
      }).toList();

      // Run inference in isolate
      await interpreter.run(inputs, outputs);

      return outputs;
    } catch (e, stack) {
      _logger.error('Isolate inference failed for model $modelId',
          error: e, stackTrace: stack);
      rethrow;
    }
  }

  /// Get model input shape
  List<int> getInputShape(String modelId) {
    final interpreter = _interpreters[modelId];
    if (interpreter == null) {
      throw Exception('Model $modelId not loaded');
    }

    return interpreter.getInputTensor(0).shape;
  }

  /// Get model output shape
  List<int> getOutputShape(String modelId) {
    final interpreter = _interpreters[modelId];
    if (interpreter == null) {
      throw Exception('Model $modelId not loaded');
    }

    return interpreter.getOutputTensor(0).shape;
  }

  /// Release a model from memory
  void releaseModel(String modelId) {
    final interpreter = _interpreters.remove(modelId);
    interpreter?.close();

    final isolateInterpreter = _isolateInterpreters.remove(modelId);
    isolateInterpreter?.close();

    _logger.info('Released model: $modelId');
  }

  /// Release all models
  void releaseAllModels() {
    for (final interpreter in _interpreters.values) {
      interpreter.close();
    }
    _interpreters.clear();

    for (final interpreter in _isolateInterpreters.values) {
      interpreter.close();
    }
    _isolateInterpreters.clear();

    _logger.info('Released all models');
  }

  /// Get hardware acceleration status
  Map<String, bool> getAccelerationStatus() {
    return {
      'gpu': _useGpuDelegate,
      'nnapi': _useNnApiDelegate,
      'coreml': _useCoreMLDelegate,
      'xnnpack': _useXNNPackDelegate,
    };
  }

  /// Log model information
  void _logModelInfo(Interpreter interpreter, String modelId) {
    try {
      final inputTensor = interpreter.getInputTensor(0);
      final outputTensor = interpreter.getOutputTensor(0);

      _logger.info('Model $modelId loaded successfully:');
      _logger.info('  Input shape: ${inputTensor.shape}');
      _logger.info('  Input type: ${inputTensor.type}');
      _logger.info('  Output shape: ${outputTensor.shape}');
      _logger.info('  Output type: ${outputTensor.type}');
    } catch (e) {
      _logger.warning('Could not log model info: $e');
    }
  }
}